# üîó Opciones para Conectar Backend con Vercel

Tienes **2 opciones** dependiendo de si quieres probar r√°pido o tener una soluci√≥n permanente.

---

## üöÄ Opci√≥n 1: T√∫nel Temporal (Para Pruebas R√°pidas)

**Cu√°ndo usar:** Para probar que todo funciona antes de desplegar en Render.

**Ventajas:**
- ‚úÖ R√°pido (5 minutos)
- ‚úÖ No necesitas desplegar nada
- ‚úÖ Pruebas inmediatas

**Desventajas:**
- ‚ùå Tu PC debe estar encendido
- ‚ùå La URL cambia cada vez que reinicias el t√∫nel
- ‚ùå No es para producci√≥n
- ‚ùå Puede ser lento

---

### Paso a Paso con ngrok

#### 1. Instalar ngrok

**Mac:**
```bash
brew install ngrok
```

**Windows/Linux:**
- Descarga de [ngrok.com](https://ngrok.com/download)
- O usa: `choco install ngrok` (Windows) / `snap install ngrok` (Linux)

#### 2. Iniciar tu Backend Local

En una terminal:
```bash
cd /Users/arielsanroj/marketingagent
PYTHONPATH=/Users/arielsanroj/marketingagent python3 frontend/app.py
```

Deber√≠as ver:
```
üöÄ Starting tphagent Frontend Server...
üåê Server: http://127.0.0.1:15000
```

#### 3. Crear T√∫nel P√∫blico

En **otra terminal** (deja la anterior corriendo):
```bash
ngrok http 15000
```

Ver√°s algo como:
```
Forwarding  https://abc123.ngrok-free.app -> http://localhost:15000
```

**Copia la URL HTTPS** (ej: `https://abc123.ngrok-free.app`)

#### 4. Configurar en Vercel

1. Abre tu sitio en Vercel (ej: `https://tu-proyecto.vercel.app`)
2. Abre la consola del navegador (F12 ‚Üí Console)
3. Pega y ejecuta:
```javascript
localStorage.setItem('API_BASE', 'https://abc123.ngrok-free.app');
location.reload();
```

**‚ö†Ô∏è IMPORTANTE:** Reemplaza `abc123.ngrok-free.app` con tu URL real de ngrok.

#### 5. Probar

- Intenta hacer un an√°lisis
- Verifica en Network que las peticiones van a tu t√∫nel de ngrok

**‚ö†Ô∏è Nota:** Cada vez que reinicies ngrok, obtendr√°s una URL nueva y tendr√°s que actualizar `localStorage` de nuevo.

---

### Paso a Paso con cloudflared (Alternativa)

#### 1. Instalar cloudflared

**Mac:**
```bash
brew install cloudflared
```

**O descarga de:** [developers.cloudflare.com](https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/install-and-setup/installation)

#### 2. Iniciar tu Backend Local

Igual que con ngrok:
```bash
cd /Users/arielsanroj/marketingagent
PYTHONPATH=/Users/arielsanroj/marketingagent python3 frontend/app.py
```

#### 3. Crear T√∫nel

En otra terminal:
```bash
cloudflared tunnel --url http://localhost:15000
```

Ver√°s:
```
+--------------------------------------------------------------------------------------------+
|  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):
|  https://random-words-1234.trycloudflare.com
+--------------------------------------------------------------------------------------------+
```

**Copia la URL HTTPS**

#### 4. Configurar en Vercel

Igual que con ngrok:
```javascript
localStorage.setItem('API_BASE', 'https://random-words-1234.trycloudflare.com');
location.reload();
```

---

## üè≠ Opci√≥n 2: Desplegar en Render (Para Producci√≥n)

**Cu√°ndo usar:** Para tener tu aplicaci√≥n funcionando 24/7 sin depender de tu PC.

**Ventajas:**
- ‚úÖ Funciona 24/7 (o casi, si usas plan Free)
- ‚úÖ URL permanente
- ‚úÖ No necesitas tener tu PC encendido
- ‚úÖ Mejor para usuarios reales

**Desventajas:**
- ‚è±Ô∏è Toma 15-20 minutos configurar
- üí∞ Plan Free se "duerme" despu√©s de 15 min (Starter $7/mes siempre activo)

---

### Paso a Paso con Render

#### 1. Ve a Render.com

1. Crea cuenta o inicia sesi√≥n: [render.com](https://render.com)
2. Click en **"New +"** ‚Üí **"Web Service"**

#### 2. Conectar Repositorio

1. Conecta tu repositorio de GitHub
2. Selecciona `marketingagent`
3. Branch: `main`

#### 3. Configurar Servicio

**Basic Settings:**
- Name: `marketingagent-backend`
- Region: Elige la m√°s cercana
- Runtime: `Python 3`

**Build & Deploy:**
- Build Command: `pip install -r requirements.txt`
- Start Command: (d√©jalo vac√≠o, usa Procfile)

**Plan:** Free (para empezar)

#### 4. Variables de Entorno

Agrega en **Environment**:

```bash
EMAIL_USER=arielsanroj@carmanfe.com.co
EMAIL_PASSWORD=tu-app-password-gmail
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587

# IMPORTANTE: Ollama no funciona en Render (est√° en localhost)
# Usa OpenAI o despliega Ollama en otro servidor
OPENAI_API_KEY=sk-tu-key-aqui
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL_NAME=gpt-4

PINECONE_API_KEY=pcsk_5F4Kxb_EpxvT1nERpkNoUPo5uNk7UiARTGpB6GRJ5TKSpApKpoYRguLGb89WHvnRupSVh
PINECONE_ENVIRONMENT=us-east-1

GOOGLE_ADS_DEVELOPER_TOKEN=sbiVN_iPT4c8oDN6dzSQnA
GOOGLE_ADS_CLIENT_ID=YOUR_GOOGLE_ADS_CLIENT_ID
GOOGLE_ADS_CLIENT_SECRET=YOUR_GOOGLE_ADS_CLIENT_SECRET
GOOGLE_ADS_REFRESH_TOKEN=YOUR_GOOGLE_ADS_REFRESH_TOKEN
GOOGLE_ADS_LOGIN_CUSTOMER_ID=7700381649
```

#### 5. Deploy

1. Click **"Create Web Service"**
2. Espera 5-10 minutos
3. Copia la URL: `https://marketingagent-backend.onrender.com`

#### 6. Conectar con Vercel

Edita `frontend/templates/index.html` l√≠nea **836**:

```javascript
const BACKEND_URL = 'https://marketingagent-backend.onrender.com'; // üëà TU URL
```

Luego:
```bash
git add frontend/templates/index.html
git commit -m "Conectar con backend de Render"
git push origin main
```

Espera 2 minutos y ¬°listo!

---

## ü§î ¬øCu√°l Opci√≥n Elegir?

### Usa T√∫nel (Opci√≥n 1) si:
- ‚úÖ Solo quieres probar r√°pido
- ‚úÖ Est√°s desarrollando y probando cambios
- ‚úÖ No necesitas que funcione 24/7
- ‚úÖ Tu PC puede estar encendida

### Usa Render (Opci√≥n 2) si:
- ‚úÖ Quieres que funcione para usuarios reales
- ‚úÖ No quieres depender de tu PC
- ‚úÖ Necesitas una soluci√≥n permanente
- ‚úÖ Quieres una URL estable

---

## ‚ö†Ô∏è Problema Com√∫n: Ollama en Localhost

**Tu backend usa Ollama** (`OLLAMA_BASE_URL=http://localhost:11434`), que est√° en tu PC.

**Esto NO funcionar√° en Render** porque Render no puede acceder a tu localhost.

### Soluciones:

#### Soluci√≥n 1: Usar OpenAI (Recomendado para Render)

1. Obt√©n API key: [platform.openai.com](https://platform.openai.com)
2. En Render, agrega:
   ```bash
   OPENAI_API_KEY=sk-tu-key
   OPENAI_API_BASE=https://api.openai.com/v1
   OPENAI_MODEL_NAME=gpt-4
   ```
3. El c√≥digo ya detecta autom√°ticamente si usar OpenAI u Ollama

#### Soluci√≥n 2: Desplegar Ollama en Otro Servidor

1. Despliega Ollama en Render/Railway/Fly.io
2. Cambia `OLLAMA_BASE_URL` a la URL p√∫blica de ese servidor

#### Soluci√≥n 3: Usar T√∫nel para Ollama tambi√©n

1. Crea un t√∫nel para Ollama: `ngrok http 11434`
2. Usa esa URL en `OLLAMA_BASE_URL`

---

## üìù Resumen R√°pido

**Opci√≥n R√°pida (T√∫nel):**
```bash
# Terminal 1: Backend
python3 frontend/app.py

# Terminal 2: T√∫nel
ngrok http 15000

# En Vercel (consola):
localStorage.setItem('API_BASE', 'https://tu-url-ngrok');
location.reload();
```

**Opci√≥n Producci√≥n (Render):**
1. Render.com ‚Üí New Web Service
2. Configura variables
3. Deploy (10 min)
4. Pega URL en `index.html` l√≠nea 836
5. `git push`

---

## ‚úÖ Checklist

**Para T√∫nel:**
- [ ] Backend corriendo en localhost:15000
- [ ] T√∫nel activo (ngrok o cloudflared)
- [ ] URL copiada
- [ ] `localStorage` configurado en Vercel
- [ ] Prueba funcionando

**Para Render:**
- [ ] Backend desplegado en Render
- [ ] Health check funciona: `/health`
- [ ] Variables de entorno configuradas
- [ ] URL pegada en `index.html`
- [ ] Cambios en Git
- [ ] Vercel re-desplegado
- [ ] Prueba funcionando

---

¬øCu√°l opci√≥n prefieres usar? Te gu√≠o paso a paso con la que elijas.

